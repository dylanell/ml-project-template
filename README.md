# ML Project Template

Template for machine learning (ML) projects and API serving. 

## Using This Template

To start a new project from this template, rename the package directory and update all references:

1. Rename `src/ml_project_template/` to `src/<your_project_name>/`
2. Update `pyproject.toml`:
   - `name` (line 2) — the installable package name (use hyphens, e.g. `my-project`)
   - `[project.scripts]` entry — the CLI name and module path
3. Find and replace `ml_project_template` with `your_project_name` in all Python files (imports)
4. Update `CLAUDE.md` and this `README.md`

Then reinstall:
```bash
uv pip install -e "."
```

## Setup

```bash
# Install uv if you haven't
curl -LsSf https://astral.sh/uv/install.sh | sh

# Create virtual environment and install dependencies
uv venv
source .venv/bin/activate
uv pip install -e "." --group dev
```

Configure VSCode notebooks (add to .vscode/settings.json)
```
{                                                                                                                                                           
  "jupyter.notebookFileRoot": "${workspaceFolder}"                                                                                                          
}
```

## Architecture

```
src/ml_project_template/
├── data/                               # Dataset abstractions
│   ├── base.py                         # BaseDataset ABC
│   ├── tabular.py                      # TabularDataset for numerical data
│   └── sequence.py                     # SequenceDataset for text/token sequences
├── models/                             # Model implementations
│   ├── base.py                         # BaseModel ABC (MLflow, save/load)
│   ├── pytorch_base.py                 # BasePytorchModel ABC (Fabric, predict, weights)
│   ├── registry.py                     # ModelRegistry for model discovery
│   ├── gb_classifier.py                # Sklearn GradientBoosting wrapper
│   ├── mlp_classifier.py               # PyTorch MLP classifier
│   └── cnn_sequence_classifier.py      # PyTorch CNN sequence classifier
├── modules/                            # Reusable nn.Module building blocks
│   ├── fully_connected.py              # FullyConnected (MLP block with norm/activation)
│   └── sequence_cnn.py                 # SequenceCNN (1D CNN via Conv2d + linear head)
├── serving/
│   └── iris_classifier.py              # FastAPI app factory for iris classification
├── utils/
│   ├── io.py                           # S3-compatible I/O utilities
│   └── seed.py                         # seed_everything() for reproducibility

configs/                                # Training configs (JSON)
docker/                                 # Dockerfiles per pipeline stage
├── preprocess-iris-dataset/Dockerfile  # Preprocessing image
├── train-iris-classifier/Dockerfile    # Training image
└── serve-iris-classifier/Dockerfile    # Serving image
argo/                                   # Argo Workflow pipelines
scripts/                                # Data onboarding, preprocessing + training scripts
notebooks/                              # R&D notebooks
tests/                                  # Test suite (no external services needed)
```

## Key Patterns

### Data Loading
```python
from ml_project_template.data import TabularDataset
from ml_project_template.utils import get_storage_options

dataset = TabularDataset.from_csv("s3://data/iris/iris.csv", target_column="species", storage_options=get_storage_options("s3://data/iris/iris.csv"))
train_data, test_data = dataset.split(test_size=0.2, random_state=42)
```

### Model Registry
```python
from ml_project_template.models import ModelRegistry
ModelRegistry.list()  # ['gb_classifier', 'mlp_classifier', 'cnn_sequence_classifier']
model = ModelRegistry.get("mlp_classifier")(layer_dims=[4, 16, 3])

# Load a saved model — returns a fully instantiated model (no need to know architecture params)
model = ModelRegistry.get("mlp_classifier").load(".models/my_model")
```

### Training
```python
# BaseModel.train() handles MLflow orchestration (params, artifacts)
# Model-specific training kwargs are forwarded to _fit()
model.train(
    experiment_name="my-experiment",
    train_data=train_data,
    val_data=val_data,
    model_path=".models/my_model",  # local or s3:// path
    run_name="run-1",               # optional
    save_model="best",              # optional: "best" or "final" (works with S3 paths too)
    # Model-specific training kwargs (e.g. for MLP):
    lr=1e-3,
    weight_decay=1e-4,
    max_epochs=100,
    batch_size=32,
)

# Or train without MLflow tracking for quick iteration
model.train(train_data=train_data, tracking=False, max_epochs=10)
```

### Reproducibility

Set a top-level `"seed"` key in your config JSON to seed all random number generators (Python, NumPy, PyTorch) for reproducible runs:

```json
{
    "seed": 42,
    "data": { ... },
    "model": { ... },
    "training": { ... }
}
```

Scripts call `seed_everything(seed)` before data loading, and pass `seed=seed` to `model.train()` which re-seeds before training. The seed is logged to MLflow automatically.

```python
from ml_project_template.utils import seed_everything
seed_everything(42)  # Seeds random, numpy, and torch (if available)
```

### Model Authoring Guide

#### Steps to add a new model

1. Create `src/ml_project_template/models/my_model.py` extending `BaseModel` (from `base.py`) or `BasePytorchModel` (from `pytorch_base.py`) for PyTorch
2. Implement `_fit()`, `_save_weights()`, `_load_weights()`, and `predict()`
3. Register in `registry.py`
4. Add lifecycle and get_params tests in `tests/test_models.py`

You do **not** need to implement `get_params()` in most cases — see below.

#### BaseModel Interface

```python
class BaseModel(ABC):
    # Public API — MLflow orchestration (set experiment, start run, log params, save artifact)
    def train(self, *, experiment_name, train_data, tracking=True, **kwargs) -> None

    # Public API — saves config.json + calls _save_weights()
    def save(self, path: str) -> str

    # Public classmethod — reads config.json, instantiates model, loads weights, returns instance
    @classmethod
    def load(cls, path: str) -> BaseModel

    # Abstract — subclasses must implement
    def _fit(self, train_data, val_data=None, **kwargs) -> None
    def _save_weights(self, dir_path: str) -> None
    def _load_weights(self, dir_path: str) -> None
    def predict(self, X: np.ndarray) -> np.ndarray

    # Log to MLflow (no-op when tracking=False) — use in _fit() instead of mlflow directly
    def log_param(self, key, value) -> None
    def log_metric(self, key, value, step=None) -> None

    # Auto-populated from __init__ args via __init_subclass__ — no override needed
    # Override only if automatic capture is insufficient (e.g. sklearn **kwargs)
    def get_params(self) -> dict
```

#### BasePytorchModel

Extends `BaseModel` with Lightning Fabric and shared PyTorch boilerplate:
- `predict()` — numpy→tensor→device→inference→cpu→numpy
- `_save_weights()`/`_load_weights()` — Fabric-based state dict persistence
- `self.fabric` — initialized from constructor args (accelerator, devices, precision, etc.)

Subclasses only need to implement `_fit()` and set `self.model`.

#### Automatic `__init__` param capture

`BaseModel` uses `__init_subclass__` to automatically record all `__init__` arguments into `self._model_params`. This means `get_params()` works out of the box — you don't need to build param dicts manually or override it.

**How it works:**

1. `BaseModel.__init_subclass__` wraps every subclass `__init__` with a decorator.
2. After the original `__init__` runs, the wrapper inspects the method signature with `inspect.signature()`, binds the actual call arguments (including defaults) via `sig.bind()` + `apply_defaults()`, and stores them in `self._model_params`.
3. This works across the inheritance chain: when `super().__init__()` is called, the parent's wrapped `__init__` runs first and captures its own params. The child's wrapper then merges its params on top.
4. `get_params()` returns the combined `_model_params` dict. `train()` logs it to MLflow automatically.

See the implementation in `src/ml_project_template/models/base.py` lines 35–74.

**Example — what happens when `MLPClassifier(layer_dims=[4, 8, 3])` is created:**

1. `MLPClassifier.__init__` calls `super().__init__()` with default Fabric args
2. `BasePytorchModel.__init__` (wrapped) runs → captures `accelerator="auto"`, `strategy="auto"`, `devices="auto"`, `precision="32-true"`, `plugins=None`, `callbacks=None`, `loggers=None` into `self._model_params`
3. `MLPClassifier.__init__` (wrapped) finishes → merges in `layer_dims=[4, 8, 3]`, `hidden_activation="ReLU"`, `output_activation="Identity"`, `use_bias=True`, `norm=None`
4. `model.get_params()` returns the combined dict of all 12 params

**When to override `get_params()`:**

Override when auto-capture is insufficient — specifically when your `__init__` uses `**kwargs` to forward arguments to an underlying library. Auto-capture will record the explicitly passed kwargs, but won't capture the library's internal defaults.

Example: `GBClassifier.__init__(self, **kwargs)` forwards to sklearn's `GradientBoostingClassifier(**kwargs)`. If you create `GBClassifier(n_estimators=200)`, auto-capture only sees `{"n_estimators": 200}`. But sklearn has dozens of other params with defaults (`learning_rate=0.1`, `max_depth=3`, etc.) that are important for reproducibility. So `GBClassifier` overrides `get_params()` to delegate to `self.model.get_params()`, which returns the full set.

**What NOT to worry about — training params:**

Training-time arguments like `lr`, `batch_size`, `max_epochs` are passed to `_fit()`, not `__init__()`, so they are **not** auto-captured. These are logged manually inside `_fit()` using `self.log_param()`. This is by design: `__init__` params define the model architecture (what gets saved/loaded), while training params are run-specific.

## Quick Start

### Local Services (MinIO + MLflow)

Start MinIO (S3-compatible storage) and MLflow (experiment tracking) via Docker Compose:

```bash
docker compose up -d
```

- **MLflow UI:** [http://localhost:5000](http://localhost:5000)
- **MinIO Console:** [http://localhost:7001](http://localhost:7001) (login: `minioadmin`/`minioadmin`)

Create `data` and `models` buckets in the MinIO console, then onboard data:

```bash
uv run python scripts/onboard_iris_dataset.py --dest s3://data/iris/
```

Stop services with `docker compose down`. Data persists in Docker volumes across restarts.

### Docker

```bash
# Build images
docker build -t preprocessing-job -f docker/preprocess-iris-dataset/Dockerfile .
docker build -t training-job -f docker/train-iris-classifier/Dockerfile .

# Run preprocessing (reads/writes data via S3)
docker run --env-file .env \
  -e S3_ENDPOINT_URL=http://host.docker.internal:7000 \
  -e MLFLOW_TRACKING_URI=http://host.docker.internal:5000 \
  -e MLFLOW_S3_ENDPOINT_URL=http://host.docker.internal:7000 \
  -v $(pwd)/configs:/app/configs \
  preprocessing-job --config configs/iris_mlp_classifier.json

# Run training (reads data via S3, saves model to S3, logs to MLflow)
docker run --env-file .env \
  -e S3_ENDPOINT_URL=http://host.docker.internal:7000 \
  -e MLFLOW_TRACKING_URI=http://host.docker.internal:5000 \
  -e MLFLOW_S3_ENDPOINT_URL=http://host.docker.internal:7000 \
  -v $(pwd)/configs:/app/configs \
  training-job --config configs/iris_mlp_classifier.json
```

> `--env-file .env` loads S3 and MLflow credentials. The `-e` flags override the endpoint URLs to use `host.docker.internal`, which resolves to the host machine from inside Docker containers (Mac/Windows). On Linux, add `--add-host=host.docker.internal:host-gateway` to the `docker run` command.

### Model Serving

Serve a trained model via FastAPI. The server reads the same JSON config used for training to determine which model to load.

```bash
# Local (requires a trained model saved to the configured model_path)
uv run python scripts/serve_iris_classifier.py --config configs/iris_mlp_classifier.json

# Docker
docker build -t serving-job -f docker/serve-iris-classifier/Dockerfile .

docker run --env-file .env \
  -e S3_ENDPOINT_URL=http://host.docker.internal:7000 \
  -p 8000:8000 \
  -v $(pwd)/configs:/app/configs \
  serving-job --config configs/iris_mlp_classifier.json
```

Test the endpoints:

```bash
curl http://localhost:8000/health
curl http://localhost:8000/info
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{"features": [[5.1, 3.5, 1.4, 0.2]]}'
```

Auto-generated API docs are available at `http://localhost:8000/docs`.

### Argo Workflows (Local)

Run the full pipeline (preprocess → train) as an Argo Workflow DAG on Docker Desktop's K8s cluster. See [argo/README.md](argo/README.md) for details.

```bash
# First-time: install Argo Workflows
kubectl create namespace argo
kubectl apply -n argo --server-side -f https://github.com/argoproj/argo-workflows/releases/latest/download/quick-start-minimal.yaml

# First-time: create secret and config map in argo namespace
source .env && kubectl create secret generic s3-credentials --namespace argo \
  --from-literal=AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \
  --from-literal=AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
kubectl create configmap training-configs --namespace argo --from-file=configs/

# Submit a pipeline (uses MLP config by default)
argo submit -n argo argo/iris-classifier-pipeline.yaml --watch

# Or specify a different config
argo submit -n argo argo/iris-classifier-pipeline.yaml -p config=configs/iris_gb_classifier.json --watch

# Argo UI (optional)
kubectl port-forward -n argo svc/argo-server 2746:2746
# Then open https://localhost:2746
```

To update configs after changing them locally:

```bash
kubectl create configmap training-configs --namespace argo --from-file=configs/ --dry-run=client -o yaml | kubectl apply -f -
```

## Testing

Run the test suite with:

```bash
uv run pytest tests/ -v
```

Tests use `tracking=False` to skip MLflow, so no external services (MLflow, MinIO) are needed. They run entirely from in-memory numpy data.

When adding a new model, add matching tests in `tests/test_models.py` following the existing pattern:
- **`test_lifecycle`** — create, train (with `tracking=False`), predict (check output shape), save to a temp dir, load into a fresh instance, predict again (outputs match)
- **`test_get_params`** — verify `get_params()` returns the expected keys and values

## ToDo

- [x] **Add tests.** Test suite in `tests/` covers model lifecycle, registry, and dataset operations.
- [x] **Save model metadata alongside artifacts.** `model.save(path)` creates a directory with `config.json` (model name + params) and weights. `ModelRegistry.load(path)` reconstructs any model from just a directory path.
- [x] **Decouple MLflow from the training path.** `train(tracking=False)` skips all MLflow calls. Models use `self.log_param()`/`self.log_metric()` helpers that respect the flag.
- [x] **Add CI/CD.** GitHub Actions workflow runs tests on PRs to main, with branch protection requiring checks to pass.
- [x] **Move BasePytorchModel to its own file.** `base.py` imports torch and lightning unconditionally, so even sklearn-only usage pays for the full PyTorch import chain. Splitting BasePytorchModel out would fix this.
- [x] **Document the `__init_subclass__` param capture for onboarding.** The auto-capture of `_model_params` is non-obvious to newcomers. Add an explanation to the contributing guide or model-authoring docs.
- [x] **Add central seed management for reproducibility.** `seed_everything()` utility seeds Python, NumPy, and PyTorch. Configs have a top-level `"seed"` key; scripts seed early and pass to `model.train(seed=...)`.
- [x] **HuggingFace-style model loading.** `ModelClass.load(path)` classmethod reads `config.json`, instantiates the model, loads weights, and returns the instance — no need for the caller to know the architecture params upfront.
- [x] **Mid-training checkpointing.** `train(save_model="best"|"final", model_path=...)` saves on best val loss or at end of training. Works with both local and S3 paths (S3: checkpoints locally, uploads once after training).
- [ ] **Consider decorator-based model registration.** Currently adding a model requires editing two files (model file + registry.py), which causes merge conflicts when multiple people add models simultaneously.
- [x] **Add API serving.** FastAPI server for model inference. Load a saved model from config, expose `/health`, `/info`, and `/predict` endpoints, containerized for deployment.
- [ ] **Support regression tasks in Dataset.** `y` is always cast to `long()` and `LabelEncoder` is built in, so the data layer is classification-only. Regression would need a new dataset class or modifications.
